{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "air_passengers_starting_kit.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeeplearning-fr/scikit-learn/blob/main/air_passengers_starting_kit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8FP92mNsWm9"
      },
      "source": [
        "# <a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science</a>\n",
        "# <a href=https://www.ramp.studio/problems/air_passengers>RAMP</a> on predicting the number of air passengers\n",
        "\n",
        "<i> Balázs Kégl (LAL/CNRS), Alex Gramfort (Inria), Djalel Benbouzid (UPMC), Mehdi Cherti (LAL/CNRS) </i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzAKvHaXsWm9"
      },
      "source": [
        "## Introduction\n",
        "The data set was donated to us by an unnamed company handling flight ticket reservations. The data is thin, it contains\n",
        "<ul>\n",
        "<li> the date of departure\n",
        "<li> the departure airport\n",
        "<li> the arrival airport\n",
        "<li> the mean and standard deviation of the number of weeks of the reservations made before the departure date\n",
        "<li> a field called <code>log_PAX</code> which is related to the number of passengers (the actual number were changed for privacy reasons)\n",
        "</ul>\n",
        "\n",
        "The goal is to predict the <code>log_PAX</code> column. The prediction quality is measured by RMSE. \n",
        "\n",
        "The data is obviously limited, but since data and location informations are available, it can be joined to external data sets. <b>The challenge in this RAMP is to find good data that can be correlated to flight traffic</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-kj1orSsWm9"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import importlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu6lskAJtE4z"
      },
      "source": [
        "# fil='/content/drive/MyDrive/COURS_ML/X executive ML/X_3/D9/air_passengers_dsca_02.zip (Unzipped Files)/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdF1YAbptGhk",
        "outputId": "6c3774a9-2e04-4ede-ecce-57db519b3ab0"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# !git clone https://github.com/ramp-kits/air_passengers.git\n",
        "%cd air_passengers\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'air_passengers'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 239 (delta 5), reused 10 (delta 2), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (239/239), 966.01 KiB | 398.00 KiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n",
            "/content/air_passengers\n",
            "Collecting ramp-workflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/df/e1ce680e6288b0eca44fa2b8b48262bf9d5b9426d943d3389098f5d80d17/ramp_workflow-0.3.3-py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.17.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: holidays in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.10.3)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ramp-workflow->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ramp-workflow->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.10.0)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.6/dist-packages (from holidays->-r requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: convertdate in /usr/local/lib/python3.6/dist-packages (from holidays->-r requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from holidays->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->-r requirements.txt (line 9)) (1.50)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.6 in /usr/local/lib/python3.6/dist-packages (from convertdate->holidays->-r requirements.txt (line 8)) (0.3.7)\n",
            "Installing collected packages: ramp-workflow\n",
            "Successfully installed ramp-workflow-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051on9K73kHB"
      },
      "source": [
        "# !pip install -U -r requirements.txt\n",
        "# !cd '/content/drive/MyDrive/COURS_ML/X executive ML/X_3/D9/air_passengers_dsca_02.zip (Unzipped Files)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ0FvyyJsWm-"
      },
      "source": [
        "## Load the dataset using pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5maq8bZQsWm-"
      },
      "source": [
        "The training and testing data are located in the folder `data`. They are compressed `csv` file (i.e. `csv.bz2`). We can load the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l_zGO5gsWm-"
      },
      "source": [
        "data = pd.read_csv(\n",
        "    os.path.join('data', 'train.csv.bz2')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygspviQgsWm-",
        "outputId": "d7b043a8-a142-4b40-b31d-821383524f2f"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8902 entries, 0 to 8901\n",
            "Data columns (total 6 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   DateOfDeparture   8902 non-null   object \n",
            " 1   Departure         8902 non-null   object \n",
            " 2   Arrival           8902 non-null   object \n",
            " 3   WeeksToDeparture  8902 non-null   float64\n",
            " 4   log_PAX           8902 non-null   float64\n",
            " 5   std_wtd           8902 non-null   float64\n",
            "dtypes: float64(3), object(3)\n",
            "memory usage: 417.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2CFRlrPsWnA"
      },
      "source": [
        "So as stated earlier, the column `log_PAX` is the target for our regression problem. The other columns are the features which will be used for our prediction problem. If we focus on the data type of the column, we can see that `DateOfDeparture`, `Departure`, and `Arrival` are of `object` dtype, meaning they are strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohIo7ZiUsWnA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "280429d4-cbbf-4f3c-92cd-826855583175"
      },
      "source": [
        "data[['DateOfDeparture', 'Departure', 'Arrival']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DateOfDeparture</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-06-19</td>\n",
              "      <td>ORD</td>\n",
              "      <td>DFW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-09-10</td>\n",
              "      <td>LAS</td>\n",
              "      <td>DEN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-10-05</td>\n",
              "      <td>DEN</td>\n",
              "      <td>LAX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-10-09</td>\n",
              "      <td>ATL</td>\n",
              "      <td>ORD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-02-21</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SFO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  DateOfDeparture Departure Arrival\n",
              "0      2012-06-19       ORD     DFW\n",
              "1      2012-09-10       LAS     DEN\n",
              "2      2012-10-05       DEN     LAX\n",
              "3      2011-10-09       ATL     ORD\n",
              "4      2012-02-21       DEN     SFO"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ML3d8TsWnA"
      },
      "source": [
        "While it makes `Departure` and `Arrival` are the code of the airport, we see that the `DateOfDeparture` should be a date instead of string. We can use pandas to convert this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrsutVnrsWnA"
      },
      "source": [
        "data.loc[:, 'DateOfDeparture'] = pd.to_datetime(data.loc[:, 'DateOfDeparture'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wT2PGQfsWnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0b7c18-bc00-4c3b-de66-cb455c7da5e5"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8902 entries, 0 to 8901\n",
            "Data columns (total 6 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   DateOfDeparture   8902 non-null   datetime64[ns]\n",
            " 1   Departure         8902 non-null   object        \n",
            " 2   Arrival           8902 non-null   object        \n",
            " 3   WeeksToDeparture  8902 non-null   float64       \n",
            " 4   log_PAX           8902 non-null   float64       \n",
            " 5   std_wtd           8902 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(3), object(2)\n",
            "memory usage: 417.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGQNOup3sWnA"
      },
      "source": [
        "When you will create a submission, `ramp-workflow` will load the data for you and split into a data matrix `X` and a target vector `y`. It will also take care about splitting the data into a training and testing set. These utilities are available in the module `problem.py` which we will load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTTd0-J6sWnA"
      },
      "source": [
        "import problem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cYDdNJsWnA"
      },
      "source": [
        "The function `get_train_data()` loads the training data and returns a pandas dataframe `X` and a numpy vector `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEa2saUCsWnA"
      },
      "source": [
        "X, y = problem.get_train_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37CUJobsWnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9697b2-9bb3-472e-ad3f-1e69d76c7c05"
      },
      "source": [
        "type(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQV5VqWzsWnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfca0a9-4b53-4c96-8ef8-4d3610c995c7"
      },
      "source": [
        "type(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo9FVtnmsWnA"
      },
      "source": [
        "We can check the information of the data `X`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFqs96RKsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a51ffa-91be-4e66-dd7c-a653813941f7"
      },
      "source": [
        "X.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8902 entries, 0 to 8901\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   DateOfDeparture   8902 non-null   object \n",
            " 1   Departure         8902 non-null   object \n",
            " 2   Arrival           8902 non-null   object \n",
            " 3   WeeksToDeparture  8902 non-null   float64\n",
            " 4   std_wtd           8902 non-null   float64\n",
            "dtypes: float64(2), object(3)\n",
            "memory usage: 347.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXVzToV0sWnB"
      },
      "source": [
        "Thus, this is important to see that `ramp-workflow` does not convert the `DateOfDeparture` column into a `datetime` format. Thus, keep in mind that you might need to make a conversion when prototyping your machine learning pipeline later on. Let's check some statistics regarding our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7jMhvxcsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18943f7-ec46-4224-ad60-26fe8cf9b045"
      },
      "source": [
        "print(min(X['DateOfDeparture']))\n",
        "print(max(X['DateOfDeparture']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2011-09-01\n",
            "2013-03-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY6_V4aIsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfcdbbda-9225-47fe-8310-f44640ce0893"
      },
      "source": [
        "X['Departure'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ORD', 'LAS', 'DEN', 'ATL', 'SFO', 'EWR', 'IAH', 'LAX', 'DFW',\n",
              "       'SEA', 'JFK', 'PHL', 'MIA', 'DTW', 'BOS', 'MSP', 'CLT', 'MCO',\n",
              "       'PHX', 'LGA'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJq4_SJWsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "9ac856e2-eec7-46ee-b23e-bbdb9cda5c3c"
      },
      "source": [
        "_ = plt.hist(y, bins=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "018c3HoLsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c9363486-85fc-49ea-ba60-9cba92a16fd7"
      },
      "source": [
        "_ = X.hist('std_wtd', bins=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJvkWk60sWnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c3522465-28c9-4835-d101-4892b0495367"
      },
      "source": [
        "_ = X.hist('WeeksToDeparture', bins=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW7ElEQVR4nO3df7RlZX3f8fdHQKWgjIC54kAco1RrnCWaqWhD7FVWrIAJtBViQiNQzNgsNFinraOrrUmXSXClhGB1qROxjNaALH+UWYqNFL0isaJgkFHR5UiHzAwDE37qxWgc/faP84w93Lk/zsw9995z932/1pp19n72r2fvOfdznvPsHydVhSSpWx6z1BWQJA2f4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuGtZSFJJnrnU9ZCWC8Nd85LkLUk+PaXsOzOUvXoR6/XpJJPt34+T/H3f+HtnWa6SPNLmuz/JDUl+Y7HqPU19xpPsXKrta/k6dKkroGXvRmBjkkOq6idJjgMOA54/peyZbd5FUVWn7RtOciWws6r+44CLP6+qtiU5FjgNeFeSZ1fVHyxAVWeUZN5/n0kOraq9w6iPlhdb7pqvr9AL85Pa+K8AnwO+PaXsu8AjSa5IsjvJriRvT3LIvhUl+ddJ7kjyYJK/TPK06TaY5JQkO1qrNkkuS7InyfeSbE3y3NkqnOR3kmxL8kCSLUmeOt18VXVfVX0I+F3gLUmOacsfNdN+JDk/yV8leVeSh5N8K8mpfdu+oO3j95PcmeR1fdPGk+xM8uYk9wBXAZ8Gntr3reOpSa5M8vapy/WNb2/ruL0d80OTvCjJF5M8lORrScZnO0Za/gx3zUtV/T1wM/CSVvQS4AvATVPKbgSuBPbSa8U/H3g58FqAJGcCbwX+BfDkto6rpm4vySta+b+sqom2jpcA/xA4CjgHuH+m+iZ5GfDHbb7jgLuAq+fYzWvpfct9YRufcT+ak+l9mB0LvA34eJKj27Q9wCuBJwIXAJcleUHfsk8BjgaeBryG3jeHu6vqyPbv7jnqus9vAmcAq4Ax4FPA29u6/x3wsSRPHnBdWoYMdw3D5/n/Qf4r9IL5C1PKPg+cDryxqh6pqj3AZcC+fvh/A/xxVd3RuhH+CDhpSuv9bOB9wGlV9eVW9mPgCcCzgbTld89S13OBD1TVV6vqR8BbgBcnWTPTAlX1Y+A+4OgkY3PsB/QC/M+q6sdV9RF632LOaOv6VFV9t3o+D3ymHZ99fgq8rap+VFV/N8t+zOWdVbWjreNfAddV1XVV9dOquh64pe2HOspw1zDcCJzSWqdPrqrvAF8E/kkrey7wLXrdN7tb18BD9IL659o6ngZc3jftASDA6r7tvBG4pqq+vq+gqj4LvAt4N7AnyaYkT5ylrk+l11rft/wkvZb+6pkWSHIYvW8TD7R6zrYfALvq0U/ku6ttlySnJflS6xJ6iF7AHts3799W1Q9nqf+gdvQNPw04e19923ZPoffNRR1luGsY/g+9LpHfAf4KoKq+B9zdyu4G/gb4EXBsVa1q/55YVb/Y1rEDeF3ftFVVdXhVfbFvO2cDZyW5uH/jVfXOqvol4Dn0umf+/Sx1vZte2AGQ5AjgGGDXLMucSa8b5sutnrPtB8DqJOkb/3ng7iSPAz4G/FdgrKpWAdfR+xD72e5M2fZ0j219BPgHfeNPmWae/uV2AB+acmyPqKpLZtxjLXuGu+atffW/BXgTve6YfW5qZTe2rpLPAJcmeWKSxyR5RpJ/2uZ9L72Tlr8IPztpefaUTd0NnApcnOR323z/OMnJrXX9CPBDel0bM7kKuCDJSS1s/wi4uaq2T50xydFJzqX3reAdVXX/APsBvVb87yU5rO3DP6IX4o8FHgf8LbA3yWn0+utncy9wTJKj+spuA05v9XsKvW80s/kfwK8l+WdJDkny+HYS9vg5ltMyZrhrWD5PL9Ru6iv7Qivbdwnka+gF3DeBB4GP0roGquoTwDuAq5N8D/g6vZOJj1JVf0Mv4DcmeS29E5N/3tZ3F70ulj+ZqZJV9b+B/0SvBb0beAaP7i8H+FqSSWAbvROl/7aq/nPf9Bn3o7kZOJFeP/0fAq9qHwzfB34PuKYt91vAlpnq2ur7LXofSHe2LpWnAh8CvgZsp/dB85E51rGD3rePt9L7YNlB79uNf/8dFn+sQxqeJOcDr62qU5a6LlrZ/OSWpA4y3CWpg+yWkaQOGqjlnmRVko+2W6nvSPLidqb++vQeCHV9kie1eZPkne327tun3H0nSVoEA7Xck2wGvlBV70/yWHrX2L4VeKCqLkmyEXhSVb05yenAG+jdnHEycHlVnTzb+o899thas2bNPHdl6T3yyCMcccQRS12NZcFjNRiP02BW6nG69dZb76uqaR8jMWe4t+trbwN+of+uuyTfBsarand6T/2bqKpnJXlfG75q6nwzbWPdunV1yy23HPCOjZqJiQnGx8eXuhrLgsdqMB6nwazU45Tk1qpaN920QR4p+nR618b+9yTPA24FLqZ3h92+wL6H3sOJoHcbd/+tzztb2aPCPcl6YD3A2NgYExMTA+3MKJucnOzEfiwGj9VgPE6D8Tjtb5BwPxR4AfCGqro5yeXAxv4ZqqqSHNCZ2araBGyCXsu9C5+6K7X1cDA8VoPxOA3G47S/QU6o7qT3Qwc3t/GP0gv7e1t3DO11T5u+Czihb/njmf25HZKkIZsz3KvqHmBHkme1olPp3Xa9BTivlZ1H75nXtPLXtKtmXgQ8PMcjWCVJQzboz3i9Afhwu1LmTno/MvAY4JokF9J7psc5bd7r6F0psw34QZtXkrSIBgr3qroNmO6M7KnTzFvARfOslyRpHnz8gCR1kOEuSR1kuEtSBw16QlVaUms2fmra8u2XnLHINZGWB1vuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgf5Yx3qJH/cQyudLXdJ6iDDXZI6yHCXpA4y3CWpgzyhqiWxddfDnD/NSU9PeErDYctdkjpooHBPsj3J1iS3JbmllR2d5Pok32mvT2rlSfLOJNuS3J7kBQu5A5Kk/R1Iy/2lVXVSVa1r4xuBG6rqROCGNg5wGnBi+7ceeM+wKitJGsx8umXOBDa34c3AWX3lH6yeLwGrkhw3j+1Ikg5QqmrumZL/CzwIFPC+qtqU5KGqWtWmB3iwqlYl+SRwSVXd1KbdALy5qm6Zss719Fr2jI2N/dLVV189zP1aEpOTkxx55JFLXY1lYc8DD3Pv3+1fvnb1UdPOv3XXw0PZ7kzrH1W+pwazUo/TS1/60lv7elMeZdCrZU6pql1Jfg64Psm3+idWVSWZ+1Pi0ctsAjYBrFu3rsbHxw9k8ZE0MTFBF/ZjMfy3D1/LpVv3f/ttP3d82vmnu7LmYMy0/lHle2owHqf9DdQtU1W72use4BPAC4F793W3tNc9bfZdwAl9ix/fyiRJi2TOlnuSI4DHVNX32/DLgf8CbAHOAy5pr9e2RbYAr09yNXAy8HBV7V6Iyqt7Znrgl6QDM0i3zBjwiV63OocCf1FV/yvJV4BrklwI3AWc0+a/Djgd2Ab8ALhg6LWWJM1qznCvqjuB501Tfj9w6jTlBVw0lNpJkg6Kd6hKUgcZ7pLUQT44TAtqphOkG9YuckWkFcaWuyR1kOEuSR1kuEtSBxnuktRBhrskdZBXy0jMfFWPP/un5cqWuyR1kOEuSR1kuEtSB9nnrhXFRwprpbDlLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kDcxaSi8OUgaLbbcJamDbLnrgNhCl5YHW+6S1EGGuyR1kOEuSR00cLgnOSTJXyf5ZBt/epKbk2xL8pEkj23lj2vj29r0NQtTdUnSTA6k5X4xcEff+DuAy6rqmcCDwIWt/ELgwVZ+WZtPkrSIBgr3JMcDZwDvb+MBXgZ8tM2yGTirDZ/ZxmnTT23zS5IWyaCXQv4Z8B+AJ7TxY4CHqmpvG98JrG7Dq4EdAFW1N8nDbf77+leYZD2wHmBsbIyJiYmD3IXRMTk52Yn9mM2GtXvnnmkAY4cPb10Laan/P1fCe2oYPE77mzPck7wS2FNVtyYZH9aGq2oTsAlg3bp1NT4+tFUvmYmJCbqwH7M5f0jXuW9Yu5dLt47+bRbbzx1f0u2vhPfUMHic9jfIX9cvA7+e5HTg8cATgcuBVUkOba3344Fdbf5dwAnAziSHAkcB9w+95tIimO2mre2XnLGINZEOzJx97lX1lqo6vqrWAK8GPltV5wKfA17VZjsPuLYNb2njtOmfraoaaq0lSbOaz3XubwbelGQbvT71K1r5FcAxrfxNwMb5VVGSdKAOqNOzqiaAiTZ8J/DCaeb5IXD2EOomSTpI3qEqSR1kuEtSBxnuktRBo3+hsTSiZrpM0kskNQpsuUtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDPc9e0ZnpWuaTlwZa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSB80Z7kken+TLSb6W5BtJ/qCVPz3JzUm2JflIkse28se18W1t+pqF3QVJ0lSD3MT0I+BlVTWZ5DDgpiSfBt4EXFZVVyd5L3Ah8J72+mBVPTPJq4F3AL+xQPWXRs5MN4Btv+SMRa6JVrI5w72qCphso4e1fwW8DPitVr4Z+H164X5mGwb4KPCuJGnr0QjxLlSpuwZ6/ECSQ4BbgWcC7wa+CzxUVXvbLDuB1W14NbADoKr2JnkYOAa4b8o61wPrAcbGxpiYmJjXjoyCycnJZbUfG9bunXumBTJ2+NJufykczHtjub2nlorHaX8DhXtV/QQ4Kckq4BPAs+e74araBGwCWLduXY2Pj893lUtuYmKC5bQf5y9hy33D2r1cunVlPdpo+7njB7zMcntPLRWP0/4O6GqZqnoI+BzwYmBVkn1/nccDu9rwLuAEgDb9KOD+odRWkjSQQa6WeXJrsZPkcOBXgTvohfyr2mznAde24S1tnDb9s/a3S9LiGuR78XHA5tbv/hjgmqr6ZJJvAlcneTvw18AVbf4rgA8l2QY8ALx6AeotSZrFIFfL3A48f5ryO4EXTlP+Q+DsodRO6hAvkdRi8g5VSeogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqoJX18/Mr1Ey/ACSpu2y5S1IHGe6S1EF2y0hLzB/O1kKw5S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSB80Z7klOSPK5JN9M8o0kF7fyo5Ncn+Q77fVJrTxJ3plkW5Lbk7xgoXdCkvRog7Tc9wIbquo5wIuAi5I8B9gI3FBVJwI3tHGA04AT27/1wHuGXmtJ0qzmDPeq2l1VX23D3wfuAFYDZwKb22ybgbPa8JnAB6vnS8CqJMcNveaSpBkd0B2qSdYAzwduBsaqanebdA8w1oZXAzv6FtvZynb3lZFkPb2WPWNjY0xMTBxYzUfQ5OTkSO7HhrV7l7oK+xk7fDTrNUomJiZG9j01ajxO+xs43JMcCXwMeGNVfS/Jz6ZVVSWpA9lwVW0CNgGsW7euxsfHD2TxkTQxMcEo7sf5I/hUyA1r93LpVp9+MZvt546P7Htq1Hic9jfQ1TJJDqMX7B+uqo+34nv3dbe01z2tfBdwQt/ix7cySdIiGeRqmQBXAHdU1Z/2TdoCnNeGzwOu7St/Tbtq5kXAw33dN5KkRTDI9+JfBn4b2Jrktlb2VuAS4JokFwJ3Aee0adcBpwPbgB8AFwy1xpKkOc0Z7lV1E5AZJp86zfwFXDTPekmS5sE7VCWpgwx3Seogr0WTRtSajZ9iw9q9+13K6i80aRC23CWpgwx3Seogw12SOshwl6QO8oSqtMysmeFZQZ5oVT9b7pLUQYa7JHWQ3TIdMtPXdUkrj+G+DBnikuZit4wkdZDhLkkdZLeM1BFeIql+ttwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshny0gd5zNnVqY5wz3JB4BXAnuq6rmt7GjgI8AaYDtwTlU9mCTA5cDpwA+A86vqqwtT9e7zue2SDtYg3TJXAq+YUrYRuKGqTgRuaOMApwEntn/rgfcMp5qSpAMxZ7hX1Y3AA1OKzwQ2t+HNwFl95R+sni8Bq5IcN6zKSpIGc7B97mNVtbsN3wOMteHVwI6++Xa2st1MkWQ9vdY9Y2NjTExMHGRVRsfk5ORQ92PD2r1DW9eoGTu82/s3LAt5nLrwN7fPsP/2umDeJ1SrqpLUQSy3CdgEsG7duhofH59vVZbcxMQEw9yP8zvc575h7V4u3er5/Lks5HHafu74gqx3KQz7b68LDvZSyHv3dbe01z2tfBdwQt98x7cySdIiOtgmwRbgPOCS9nptX/nrk1wNnAw83Nd9I2mEeIlktw1yKeRVwDhwbJKdwNvohfo1SS4E7gLOabNfR+8yyG30LoW8YAHqLEmaw5zhXlW/OcOkU6eZt4CL5lspSdL8+PgBSeogw12SOshr0SQ9iidau8GWuyR1kOEuSR1kuEtSB9nnPgJ8tK+kYbPlLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHeTVMovIq2IkLRZb7pLUQbbcJQ1ktm+ePndm9Nhyl6QOMtwlqYPslpE0bwd6sYDdOAvPlrskdZDhLkkdZLhLUgfZ5y5p0flTfgvPcF8A3okqaakZ7vMwNcQ3rN3L+Qa7dNBs0Q+P4T4AW+KSlhvDvY8hLo0mW/QHznCX1Dl+GCxQuCd5BXA5cAjw/qq6ZCG2czBsnUvdse/v2fNd+xt6uCc5BHg38KvATuArSbZU1TeHvS3wE1qSprMQLfcXAtuq6k6AJFcDZwILEu4zsYUuaapRzIWFaoimqoa7wuRVwCuq6rVt/LeBk6vq9VPmWw+sb6PPAr491IosjWOB+5a6EsuEx2owHqfBrNTj9LSqevJ0E5bshGpVbQI2LdX2F0KSW6pq3VLXYznwWA3G4zQYj9P+FuLZMruAE/rGj29lkqRFshDh/hXgxCRPT/JY4NXAlgXYjiRpBkPvlqmqvUleD/wlvUshP1BV3xj2dkZUp7qZFpjHajAep8F4nKYY+glVSdLS83nuktRBhrskdZDhPiRJtifZmuS2JLcsdX1GRZIPJNmT5Ot9ZUcnuT7Jd9rrk5ayjqNghuP0+0l2tffUbUlOX8o6joIkJyT5XJJvJvlGkotbue+pKQz34XppVZ3k9baPciXwiillG4EbqupE4IY2vtJdyf7HCeCy9p46qaquW+Q6jaK9wIaqeg7wIuCiJM/B99R+DHctqKq6EXhgSvGZwOY2vBk4a1ErNYJmOE6aoqp2V9VX2/D3gTuA1fie2o/hPjwFfCbJre3RCprZWFXtbsP3AGNLWZkR9/okt7dumxXf1dAvyRrg+cDN+J7aj+E+PKdU1QuA0+h9VXzJUldoOajetbhejzu99wDPAE4CdgOXLm11RkeSI4GPAW+squ/1T/M91WO4D0lV7Wqve4BP0Hs6pqZ3b5LjANrrniWuz0iqqnur6idV9VPgz/E9BUCSw+gF+4er6uOt2PfUFIb7ECQ5IskT9g0DLwe+PvtSK9oW4Lw2fB5w7RLWZWTtC6vmn+N7iiQBrgDuqKo/7Zvke2oK71AdgiS/QK+1Dr1HOvxFVf3hElZpZCS5Chin90jWe4G3Af8TuAb4eeAu4JyqWtEnE2c4TuP0umQK2A68rq9feUVKcgrwBWAr8NNW/FZ6/e6+p/oY7pLUQXbLSFIHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskddD/AyLiTN3EEQjtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjnFQQUfsWnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "968bcd3d-a6ea-4779-d158-187922d30ddc"
      },
      "source": [
        "X.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WeeksToDeparture</th>\n",
              "      <th>std_wtd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8902.000000</td>\n",
              "      <td>8902.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>11.446469</td>\n",
              "      <td>8.617773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.787140</td>\n",
              "      <td>2.139604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.625000</td>\n",
              "      <td>2.160247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.523810</td>\n",
              "      <td>7.089538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>11.300000</td>\n",
              "      <td>8.571116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13.240000</td>\n",
              "      <td>10.140521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>23.163265</td>\n",
              "      <td>15.862216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       WeeksToDeparture      std_wtd\n",
              "count       8902.000000  8902.000000\n",
              "mean          11.446469     8.617773\n",
              "std            2.787140     2.139604\n",
              "min            2.625000     2.160247\n",
              "25%            9.523810     7.089538\n",
              "50%           11.300000     8.571116\n",
              "75%           13.240000    10.140521\n",
              "max           23.163265    15.862216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwdjLFo1sWnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbae8f40-0a53-4c11-93cf-d59e099c601d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8902, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOmJ-Ur0sWnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d3e732-2dd4-4a28-a800-28c68723fde1"
      },
      "source": [
        "print(y.mean())\n",
        "print(y.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.99904767212102\n",
            "0.9938894125318564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pF590qhJTen"
      },
      "source": [
        "# !pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCun9-2QsWnC"
      },
      "source": [
        "## Preprocessing dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3gSt459sWnC"
      },
      "source": [
        "Getting dates into numerical columns is a common operation when time series data is analyzed with non-parametric predictors. The code below makes the following transformations:\n",
        "\n",
        "- numerical columns for year (2011-2012), month (1-12), day of the month (1-31), day of the week (0-6), and week of the year (1-52)\n",
        "- number of days since 1970-01-01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaMJ84kVsWnC"
      },
      "source": [
        "# Make a copy of the original data to avoid writing on the original data\n",
        "X_encoded = X.copy()\n",
        "\n",
        "# following http://stackoverflow.com/questions/16453644/regression-with-date-variable-using-scikit-learn\n",
        "X_encoded['DateOfDeparture'] = pd.to_datetime(X_encoded['DateOfDeparture'])\n",
        "X_encoded['year'] = X_encoded['DateOfDeparture'].dt.year\n",
        "X_encoded['month'] = X_encoded['DateOfDeparture'].dt.month\n",
        "X_encoded['day'] = X_encoded['DateOfDeparture'].dt.day\n",
        "X_encoded['weekday'] = X_encoded['DateOfDeparture'].dt.weekday\n",
        "X_encoded['week'] = X_encoded['DateOfDeparture'].dt.isocalendar().week\n",
        "X_encoded['n_days'] = X_encoded['DateOfDeparture'].apply(lambda date: (date - pd.to_datetime(\"1970-01-01\")).days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e3keTAisWnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f96a1f4-8626-48c4-92b2-19dae7f278ed"
      },
      "source": [
        "X_encoded.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DateOfDeparture</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>WeeksToDeparture</th>\n",
              "      <th>std_wtd</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>week</th>\n",
              "      <th>n_days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8897</th>\n",
              "      <td>2011-10-02</td>\n",
              "      <td>DTW</td>\n",
              "      <td>ATL</td>\n",
              "      <td>9.263158</td>\n",
              "      <td>7.316967</td>\n",
              "      <td>2011</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>39</td>\n",
              "      <td>15249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8898</th>\n",
              "      <td>2012-09-25</td>\n",
              "      <td>DFW</td>\n",
              "      <td>ORD</td>\n",
              "      <td>12.772727</td>\n",
              "      <td>10.641034</td>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>15608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8899</th>\n",
              "      <td>2012-01-19</td>\n",
              "      <td>SFO</td>\n",
              "      <td>LAS</td>\n",
              "      <td>11.047619</td>\n",
              "      <td>7.908705</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>15358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8900</th>\n",
              "      <td>2013-02-03</td>\n",
              "      <td>ORD</td>\n",
              "      <td>PHL</td>\n",
              "      <td>6.076923</td>\n",
              "      <td>4.030334</td>\n",
              "      <td>2013</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>15739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8901</th>\n",
              "      <td>2011-11-26</td>\n",
              "      <td>DTW</td>\n",
              "      <td>ATL</td>\n",
              "      <td>9.526316</td>\n",
              "      <td>6.167733</td>\n",
              "      <td>2011</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>5</td>\n",
              "      <td>47</td>\n",
              "      <td>15304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     DateOfDeparture Departure Arrival  WeeksToDeparture    std_wtd  year  \\\n",
              "8897      2011-10-02       DTW     ATL          9.263158   7.316967  2011   \n",
              "8898      2012-09-25       DFW     ORD         12.772727  10.641034  2012   \n",
              "8899      2012-01-19       SFO     LAS         11.047619   7.908705  2012   \n",
              "8900      2013-02-03       ORD     PHL          6.076923   4.030334  2013   \n",
              "8901      2011-11-26       DTW     ATL          9.526316   6.167733  2011   \n",
              "\n",
              "      month  day  weekday  week  n_days  \n",
              "8897     10    2        6    39   15249  \n",
              "8898      9   25        1    39   15608  \n",
              "8899      1   19        3     3   15358  \n",
              "8900      2    3        6     5   15739  \n",
              "8901     11   26        5    47   15304  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaAe5iy8sWnC"
      },
      "source": [
        "We will perform all preprocessing steps within a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/compose.html) which chains together tranformation and estimator steps. This offers offers convenience and safety (help avoid leaking statistics from your test data into the trained model in cross-validation) and the whole pipeline can be evaluated with `cross_val_score`.\n",
        "\n",
        "To perform the above encoding within a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/compose.html) we will a function and using `FunctionTransformer` to make it compatible with scikit-learn API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMMVaBmsWnC"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from datetime import datetime\n",
        "def _encode_dates(X):\n",
        "    # With pandas < 1.0, we wil get a SettingWithCopyWarning\n",
        "    # In our case, we will avoid this warning by triggering a copy\n",
        "    # More information can be found at:\n",
        "    # https://github.com/scikit-learn/scikit-learn/issues/16191\n",
        "    X_encoded = X.copy()\n",
        "\n",
        "    # Make sure that DateOfDeparture is of datetime format\n",
        "    X_encoded.loc[:, 'DateOfDeparture'] = pd.to_datetime(X_encoded['DateOfDeparture'])\n",
        "    # Encode the DateOfDeparture\n",
        "    X_encoded.loc[:, 'year'] = X_encoded['DateOfDeparture'].dt.year\n",
        "    X_encoded.loc[:, 'month'] = X_encoded['DateOfDeparture'].dt.month\n",
        "    X_encoded.loc[:, 'day'] = X_encoded['DateOfDeparture'].dt.day\n",
        "    X_encoded.loc[:, 'weekday'] = X_encoded['DateOfDeparture'].dt.weekday\n",
        "    X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.isocalendar().week\n",
        "    X_encoded.loc[:, 'n_days'] = X_encoded['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Once we did the encoding, we will not need DateOfDeparture\n",
        "    return X_encoded.drop(columns=[\"DateOfDeparture\"])\n",
        "\n",
        "date_encoder = FunctionTransformer(_encode_dates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RtJCcgKsWnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89eff11-0cee-4f09-aa6d-8f3d7e41d5e9"
      },
      "source": [
        "date_encoder.fit_transform(X).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>WeeksToDeparture</th>\n",
              "      <th>std_wtd</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>week</th>\n",
              "      <th>n_days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORD</td>\n",
              "      <td>DFW</td>\n",
              "      <td>12.875000</td>\n",
              "      <td>9.812647</td>\n",
              "      <td>2012</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>15510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LAS</td>\n",
              "      <td>DEN</td>\n",
              "      <td>14.285714</td>\n",
              "      <td>9.466734</td>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>15593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DEN</td>\n",
              "      <td>LAX</td>\n",
              "      <td>10.863636</td>\n",
              "      <td>9.035883</td>\n",
              "      <td>2012</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>15618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ATL</td>\n",
              "      <td>ORD</td>\n",
              "      <td>11.480000</td>\n",
              "      <td>7.990202</td>\n",
              "      <td>2011</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>15256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DEN</td>\n",
              "      <td>SFO</td>\n",
              "      <td>11.450000</td>\n",
              "      <td>9.517159</td>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>15391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Departure Arrival  WeeksToDeparture   std_wtd  year  month  day  weekday  \\\n",
              "0       ORD     DFW         12.875000  9.812647  2012      6   19        1   \n",
              "1       LAS     DEN         14.285714  9.466734  2012      9   10        0   \n",
              "2       DEN     LAX         10.863636  9.035883  2012     10    5        4   \n",
              "3       ATL     ORD         11.480000  7.990202  2011     10    9        6   \n",
              "4       DEN     SFO         11.450000  9.517159  2012      2   21        1   \n",
              "\n",
              "   week  n_days  \n",
              "0    25   15510  \n",
              "1    37   15593  \n",
              "2    40   15618  \n",
              "3    40   15256  \n",
              "4     8   15391  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQadl3CrsWnC"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lte0W5wpsWnC"
      },
      "source": [
        "Tree-based algorithms requires less complex preprocessing than linear-models. We will first present a machine-learning pipeline where we will use a random forest. In this pipeline, we will need to:\n",
        "\n",
        "- encode the date to numerical values (as presented in the section above);\n",
        "- ordinal encode the other categorical values to get numerical number;\n",
        "- keep numerical features as they are.\n",
        "\n",
        "Thus, we want to perform three different processes on different columns of the original data `X`. In scikit-learn, we can use [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) to perform such processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XqMDy4qsWnC"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "date_encoder = FunctionTransformer(_encode_dates)\n",
        "date_cols = [\"DateOfDeparture\"]\n",
        "\n",
        "categorical_encoder = OrdinalEncoder()\n",
        "categorical_cols = [\"Arrival\", \"Departure\"]\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (date_encoder, date_cols),\n",
        "    (categorical_encoder, categorical_cols),\n",
        "    remainder='passthrough',  # passthrough numerical columns as they are\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGy6hi-OsWnC"
      },
      "source": [
        "We can combine our preprocessor with an estimator (`RandomForestRegressor` in this case), allowing us to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOb56d_8hgnl"
      },
      "source": [
        "# %%writefile /content/air_passengers/submissions/my_first_kit/estimator.py\n",
        "\n",
        "# from sklearn.preprocessing import OrdinalEncoder\n",
        "# from sklearn.compose import make_column_transformer\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# def get_estimator():\n",
        "#     cat_processor = OrdinalEncoder()\n",
        "#     cat_columns = [\"Departure\", \"Arrival\"]\n",
        "\n",
        "#     num_processor = \"passthrough\"\n",
        "#     num_columns = [\"WeeksToDeparture\", \"std_wtd\"]\n",
        "\n",
        "#     preprocessor = make_column_transformer(\n",
        "#         (cat_processor, cat_columns),\n",
        "#         (num_processor, num_columns),\n",
        "#         remainder=\"drop\",  # drop the unused columns\n",
        "#     )\n",
        "\n",
        "#     return make_pipeline(preprocessor, RandomForestRegressor(n_estimators=20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01oNOYTcsWnC"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        " \n",
        "def _encode_dates(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Encode the date information from the DateOfDeparture columns\n",
        "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    return X.drop(columns=[\"DateOfDeparture\"])\n",
        " \n",
        "def get_estimator():\n",
        "    date_encoder = FunctionTransformer(_encode_dates)\n",
        "    date_cols = [\"DateOfDeparture\"]\n",
        " \n",
        "    categorical_encoder = OrdinalEncoder()\n",
        "    categorical_cols = [\"Arrival\", \"Departure\"]\n",
        " \n",
        "    preprocessor = make_column_transformer(\n",
        "        (date_encoder, date_cols),\n",
        "        (categorical_encoder, categorical_cols),\n",
        "        remainder='passthrough',  # passthrough numerical columns as they are\n",
        "      )\n",
        " \n",
        "    n_estimators = 200\n",
        "    max_depth = 15\n",
        "    max_features = 10\n",
        "\n",
        "    # regressor = RandomForestRegressor(\n",
        "    #   \tn_estimators=n_estimators, max_depth=max_depth, max_features=max_features\n",
        "\t  # )\n",
        "    regressor = GradientBoostingRegressor(\n",
        "      max_depth=8,\n",
        "      n_estimators=3000,\n",
        "      learning_rate=0.05\n",
        "        )\n",
        " \n",
        "    return make_pipeline(preprocessor, regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-TB9aKbODPW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWTjX3Eqz-w"
      },
      "source": [
        "\n",
        "pipeline = get_estimator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtyt_cBysWnC"
      },
      "source": [
        "We can cross-validate our `pipeline` using `cross_val_score`. Below we will have specified `cv=5` meaning KFold cross-valdiation splitting will be used, with 8 folds. The mean squared error regression loss is calculated for each split. The output `score` will be an array of 5 scores from each KFold. The score mean and standard deviation of the 5 scores is printed at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_uC8Tf3sWnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec057f5c-f004-46f0-9516-4557157bebba"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(\n",
        "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
        ")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\n",
        "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.4088 +/- 0.0214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sdA4NYtsWnC"
      },
      "source": [
        "## Linear regressor\n",
        "\n",
        "When dealing with a linear model, we need to one-hot encode categorical variables instead of ordinal encoding and standardize numerical variables. Thus we will:\n",
        "\n",
        "- encode the date;\n",
        "- then, one-hot encode all categorical columns, including the encoded date as well;\n",
        "- standardize the numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMej6Qb8sWnC"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "date_encoder = FunctionTransformer(_encode_dates)\n",
        "date_cols = [\"DateOfDeparture\"]\n",
        "\n",
        "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "categorical_cols = [\n",
        "    \"Arrival\", \"Departure\", \"year\", \"month\", \"day\",\n",
        "    \"weekday\", \"week\", \"n_days\"\n",
        "]\n",
        "\n",
        "numerical_scaler = StandardScaler()\n",
        "numerical_cols = [\"WeeksToDeparture\", \"std_wtd\"]\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (categorical_encoder, categorical_cols),\n",
        "    (numerical_scaler, numerical_cols)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eELVcvICsWnC"
      },
      "source": [
        "We can now combine our `preprocessor` with the `LinearRegression` estimator in a `Pipeline`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40YdcUu9sWnC"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regressor = LinearRegression()\n",
        "\n",
        "pipeline = make_pipeline(date_encoder, preprocessor, regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTE7cj1ysWnC"
      },
      "source": [
        "And we can evaluate our linear-model pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Cf19rlsWnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df6db89-9a2b-4d18-a832-fdc520f79071"
      },
      "source": [
        "scores = cross_val_score(\n",
        "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
        ")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\n",
        "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.6117 +/- 0.0149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vwD0qKOsu36"
      },
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import FunctionTransformer\n",
        "# from sklearn.compose import make_column_transformer\n",
        "# from sklearn.preprocessing import OrdinalEncoder\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# from xgboost import XGBClassifier\n",
        "# import xgboost as xgb\n",
        "# from sklearn.neural_network import MLPRegressor\n",
        " \n",
        "# def _encode_dates(X):\n",
        "#     # Make sure that DateOfDeparture is of dtype datetime\n",
        "#     X = X.copy()  # modify a copy of X\n",
        "#     X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "#     # Encode the date information from the DateOfDeparture columns\n",
        "#     X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "#     X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "#     X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "#     X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "#     X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "#     X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "#         lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "#     )\n",
        "#     # Finally we can drop the original columns from the dataframe\n",
        "#     return X.drop(columns=[\"DateOfDeparture\"])\n",
        " \n",
        "# def get_estimator():\n",
        "#     date_encoder = FunctionTransformer(_encode_dates)\n",
        "#     date_cols = [\"DateOfDeparture\"]\n",
        " \n",
        "#     categorical_encoder = OrdinalEncoder()\n",
        "#     categorical_cols = [\"Arrival\", \"Departure\"]\n",
        " \n",
        "#     preprocessor = make_column_transformer(\n",
        "#         (date_encoder, date_cols),\n",
        "#         (categorical_encoder, categorical_cols),\n",
        "#         remainder='passthrough',  # passthrough numerical columns as they are\n",
        "#       )\n",
        " \n",
        "#     n_estimators = 1000\n",
        "#     max_depth = 5\n",
        "#     max_features = 6\n",
        "\n",
        "#     # regressor = RandomForestRegressor(\n",
        "#     #  \tn_estimators=n_estimators, max_depth=max_depth, max_features=max_features\n",
        "# \t  # )\n",
        "#     # regressor = Lasso(alpha=0.1)\n",
        "#     regressor = MLPRegressor(random_state=1, max_iter=500,hidden_layer_sizes=((32,256),(256,128),(128,16)))\n",
        "#     # regressor =XGBClassifier(n_estimators=256,learning_rate=0.1,max_depth=4)\n",
        "#     # regressor = GradientBoostingRegressor(\n",
        "#     #   max_depth=8,\n",
        "#     #   n_estimators=50000,\n",
        "#     #   learning_rate=0.0001\n",
        "#     #     )    \n",
        " \n",
        "#     return make_pipeline(preprocessor, regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLnBLb3cLwUj"
      },
      "source": [
        "#SOLUTION ICI la vraie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHgnFhraE_eE"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "# https://www.datacorner.fr/xgboost/\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "def _merge_external_data(X):\n",
        "    filepath = os.path.join(\n",
        "        os.path.dirname(__file__), 'external_data.csv'\n",
        "    )\n",
        "  \n",
        "    X = X.copy()  # to avoid raising SettingOnCopyWarning\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Parse date to also be of dtype datetime\n",
        "    data_weather = pd.read_csv(filepath, parse_dates=[\"Date\"])\n",
        "\n",
        "    # X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC']]\n",
        "\n",
        "    data_weather=data_weather.replace([np.inf, -np.inf], np.nan)\n",
        "    # data_weather.Precipitationmm=pd.to_numeric(data_weather.Precipitationmm, errors='coerce')\n",
        "    # data_weather=data_weather.dropna(axis='columns')    \n",
        "    # X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC','Precipitationmm']]\n",
        "    data_weather['gap_temp']=data_weather['Max TemperatureC']-data_weather['Min TemperatureC']\n",
        "    data_weather['Mean TemperatureC']=data_weather['Mean TemperatureC'].shift(-11)\n",
        "    data_weather['gap_temp']=data_weather['gap_temp'].shift(-11)\n",
        "    data_weather=data_weather.fillna(0)\n",
        "    X_weather = data_weather[['Date', 'AirPort', 'Mean TemperatureC','gap_temp']]\n",
        "    X_weather = X_weather.rename(\n",
        "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'})\n",
        "    X_merged = pd.merge(\n",
        "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
        "    )\n",
        "    return X_merged\n",
        "\n",
        " \n",
        "def _encode_dates(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Encode the date information from the DateOfDeparture columns\n",
        "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    return X.drop(columns=[\"DateOfDeparture\"])\n",
        " \n",
        "def get_estimator():\n",
        "    data_merger = FunctionTransformer(_merge_external_data)\n",
        "    date_encoder = FunctionTransformer(_encode_dates)\n",
        "    date_cols = [\"DateOfDeparture\"]\n",
        " \n",
        "    categorical_encoder = OrdinalEncoder()\n",
        "    categorical_cols = [\"Arrival\", \"Departure\"]\n",
        " \n",
        "    preprocessor = make_column_transformer(\n",
        "        (date_encoder, date_cols),\n",
        "        (categorical_encoder, categorical_cols),\n",
        "        remainder='passthrough',  # passthrough numerical columns as they are\n",
        "      )\n",
        " \n",
        "    n_estimators = 32\n",
        "    max_depth = 4\n",
        "    max_features = 17\n",
        "\n",
        "#    regressor = RandomForestRegressor(\n",
        "#      \tn_estimators=n_estimators, max_depth=max_depth, max_features=max_features\n",
        "#\t  )\n",
        "#\tregressor = MLPRegressor(random_state=1, max_iter=500,hidden_layer_sizes=((32,256),(256,128),(128,16)))\n",
        "    # regressor = GradientBoostingRegressor(\n",
        "    #   max_depth=4,\n",
        "    #   n_estimators=3000,\n",
        "    #   learning_rate=0.05\n",
        "    #     )\n",
        "    regressor = XGBRegressor(n_estimators=1000,objective=\"reg:linear\",boost='linear',max_depth= 9,learning_rate=0.1)\n",
        "    \n",
        "    return make_pipeline(data_merger,preprocessor, regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIMzisio42Iw"
      },
      "source": [
        "# test mlp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEQSkx2x4058"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "# https://www.datacorner.fr/xgboost/\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "def _merge_external_data(X):\n",
        "    filepath = os.path.join(\n",
        "        os.path.dirname(__file__), 'external_data.csv'\n",
        "    )\n",
        "  \n",
        "    X = X.copy()  # to avoid raising SettingOnCopyWarning\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Parse date to also be of dtype datetime\n",
        "    data_weather = pd.read_csv(filepath, parse_dates=[\"Date\"])\n",
        "\n",
        "    # X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC']]\n",
        "\n",
        "    # data_weather=data_weather.replace([np.inf, -np.inf], np.nan)\n",
        "    # print(data_weather.columns)\n",
        "    data_weather=pd.to_numeric(data_weather, errors='coerce')\n",
        "    data_weather=data_weather.dropna(axis='columns')    \n",
        "    data_weather=data_weather.fillna(0)\n",
        "    # X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC','Precipitationmm']]\n",
        "    # data_weather['gap_temp']=data_weather['Max TemperatureC']-data_weather['Min TemperatureC']\n",
        "    # data_weather['Mean TemperatureC']=data_weather['Mean TemperatureC'].shift(-11)\n",
        "    # data_weather['gap_temp']=data_weather['gap_temp'].shift(-11)\n",
        "\n",
        "    # X_weather = data_weather[['Date', 'AirPort', 'Mean TemperatureC','gap_temp']]\n",
        "    X_weather = data_weather\n",
        "    X_weather = X_weather.rename(\n",
        "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'})\n",
        "    print(X_weather)\n",
        "    X_merged = pd.merge(\n",
        "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
        "    )\n",
        "    # print(X_merged)\n",
        "    return X_merged\n",
        "\n",
        " \n",
        "def _encode_dates(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Encode the date information from the DateOfDeparture columns\n",
        "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    return X.drop(columns=[\"DateOfDeparture\"])\n",
        "\n",
        "def _MLP_pca(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    # print(X.columns)\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    def encoder(W_encoder,b_encoder,data):\n",
        "        res_autoencoder=data\n",
        "        for i,(w,b) in enumerate(zip(W_encoder,b_encoder)):\n",
        "          res_autoencoder=res_autoencoder@w+b    \n",
        "        if i<len(W_encoder):\n",
        "          res_autoencoder=np.maximum(0,res_autoencoder)\n",
        "        return res_autoencoder\n",
        "\n",
        "    autoencoder = MLPRegressor(random_state=1, max_iter=1000,hidden_layer_sizes=(2,20,10,2,10,10,2))\n",
        "    autoencoder.fit(X,X)\n",
        "\n",
        "    W=autoencoder.coefs_\n",
        "    b=autoencoder.intercepts_\n",
        "    W_encoder= W[:4]\n",
        "    b_encoder=b[:4]\n",
        "    res_autoencoder=encoder(W_encoder,b_encoder,X)\n",
        "    # print(res_autoencoder)\n",
        "    return res_autoencoder\n",
        "\n",
        "\n",
        "def get_estimator():\n",
        "    data_merger = FunctionTransformer(_merge_external_data)\n",
        "    date_encoder = FunctionTransformer(_encode_dates)\n",
        "    autoencoder = FunctionTransformer(_MLP_pca)\n",
        "    date_cols = [\"DateOfDeparture\"]\n",
        " \n",
        "    categorical_encoder = OrdinalEncoder()\n",
        "    categorical_cols = [\"Arrival\", \"Departure\"]\n",
        " \n",
        "    preprocessor = make_column_transformer(\n",
        "        (date_encoder, date_cols),\n",
        "        (categorical_encoder, categorical_cols),\n",
        "        remainder='passthrough',  # passthrough numerical columns as they are\n",
        "      )\n",
        "\n",
        "    regressor = XGBRegressor(n_estimators=3000,objective=\"reg:squarederror\",boost='gbtree',max_depth=10,learning_rate=0.1)\n",
        "    \n",
        "    return make_pipeline(preprocessor,regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oGQWlojk3B0"
      },
      "source": [
        "pipeline = make_pipeline(get_estimator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXUMXPBgsz2G",
        "outputId": "89a32b9f-7095-4d5d-d637-c80d7e9cac0d"
      },
      "source": [
        "scores = cross_val_score(\n",
        "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
        ")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\n",
        "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.4009 +/- 0.0178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1sF0hJHyyQK"
      },
      "source": [
        "# from sklearn.linear_model import Lasso\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# lasso = Lasso()\n",
        "# pipeline_lasso = make_pipeline(date_encoder, preprocessor, lasso)\n",
        "# pipeline_lasso_gs = GridSearchCV(\n",
        "#     pipeline_lasso,\n",
        "#     {'lasso__alpha': [1e-5, 1e-4, 1e-3, 1e-2, ]}\n",
        "# )\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10)\n",
        "# pipeline_lasso_gs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JoDDKdQtM-R"
      },
      "source": [
        "def _encode_dates(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Encode the date information from the DateOfDeparture columns\n",
        "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    print(X.columns)\n",
        "    return X.drop(columns=[\"DateOfDeparture\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTdICsHwr-2t"
      },
      "source": [
        "date_encoder = FunctionTransformer(_encode_dates)\n",
        "date_cols = [\"DateOfDeparture\"]\n",
        "\n",
        "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "categorical_cols = [\n",
        "    \"Arrival\", \"Departure\", \"year\", \"month\", \"day\",\n",
        "    \"weekday\", \"week\", \"n_days\"\n",
        "]\n",
        "preprocessor_one_hot = make_column_transformer(\n",
        "    (date_encoder, date_cols),\n",
        "    (categorical_encoder, categorical_cols),\n",
        "    remainder='passthrough',  # passthrough numerical columns as they are\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "1fotJpXJurVO",
        "outputId": "54500ac6-7f9d-4269-e85d-cc3bf1e714f1"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DateOfDeparture</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>WeeksToDeparture</th>\n",
              "      <th>std_wtd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-06-19</td>\n",
              "      <td>ORD</td>\n",
              "      <td>DFW</td>\n",
              "      <td>12.875000</td>\n",
              "      <td>9.812647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-09-10</td>\n",
              "      <td>LAS</td>\n",
              "      <td>DEN</td>\n",
              "      <td>14.285714</td>\n",
              "      <td>9.466734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-10-05</td>\n",
              "      <td>DEN</td>\n",
              "      <td>LAX</td>\n",
              "      <td>10.863636</td>\n",
              "      <td>9.035883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-10-09</td>\n",
              "      <td>ATL</td>\n",
              "      <td>ORD</td>\n",
              "      <td>11.480000</td>\n",
              "      <td>7.990202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-02-21</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SFO</td>\n",
              "      <td>11.450000</td>\n",
              "      <td>9.517159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8897</th>\n",
              "      <td>2011-10-02</td>\n",
              "      <td>DTW</td>\n",
              "      <td>ATL</td>\n",
              "      <td>9.263158</td>\n",
              "      <td>7.316967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8898</th>\n",
              "      <td>2012-09-25</td>\n",
              "      <td>DFW</td>\n",
              "      <td>ORD</td>\n",
              "      <td>12.772727</td>\n",
              "      <td>10.641034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8899</th>\n",
              "      <td>2012-01-19</td>\n",
              "      <td>SFO</td>\n",
              "      <td>LAS</td>\n",
              "      <td>11.047619</td>\n",
              "      <td>7.908705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8900</th>\n",
              "      <td>2013-02-03</td>\n",
              "      <td>ORD</td>\n",
              "      <td>PHL</td>\n",
              "      <td>6.076923</td>\n",
              "      <td>4.030334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8901</th>\n",
              "      <td>2011-11-26</td>\n",
              "      <td>DTW</td>\n",
              "      <td>ATL</td>\n",
              "      <td>9.526316</td>\n",
              "      <td>6.167733</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8902 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     DateOfDeparture Departure Arrival  WeeksToDeparture    std_wtd\n",
              "0         2012-06-19       ORD     DFW         12.875000   9.812647\n",
              "1         2012-09-10       LAS     DEN         14.285714   9.466734\n",
              "2         2012-10-05       DEN     LAX         10.863636   9.035883\n",
              "3         2011-10-09       ATL     ORD         11.480000   7.990202\n",
              "4         2012-02-21       DEN     SFO         11.450000   9.517159\n",
              "...              ...       ...     ...               ...        ...\n",
              "8897      2011-10-02       DTW     ATL          9.263158   7.316967\n",
              "8898      2012-09-25       DFW     ORD         12.772727  10.641034\n",
              "8899      2012-01-19       SFO     LAS         11.047619   7.908705\n",
              "8900      2013-02-03       ORD     PHL          6.076923   4.030334\n",
              "8901      2011-11-26       DTW     ATL          9.526316   6.167733\n",
              "\n",
              "[8902 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOIeMmMcq7_V"
      },
      "source": [
        "# from sklearn.linear_model import Lasso\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# lasso = Lasso()\n",
        "# pipeline_lasso = make_pipeline(date_encoder, preprocessor, lasso)\n",
        "# pipeline_lasso_gs = GridSearchCV(\n",
        "#     pipeline_lasso,\n",
        "#     {\"lasso__alpha\": [1e-5, 1e-4, 1e-3, 1e-2, ]},\n",
        "   \n",
        "# )\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10)\n",
        "# pipeline_lasso_gs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4U2rzlszX7-"
      },
      "source": [
        "# pipeline_lasso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ohcf1o8cx49B",
        "outputId": "7c5c1552-54af-455c-ec00-99099b266386"
      },
      "source": [
        "date_encoder = FunctionTransformer(_encode_dates)\n",
        "date_cols = [\"DateOfDeparture\"]\n",
        "\n",
        "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "categorical_cols = [\n",
        "    \"Arrival\", \"Departure\", \"year\", \"month\", \"day\",\n",
        "    \"weekday\", \"week\", \"n_days\"\n",
        "]\n",
        "preprocessor_one_hot = make_column_transformer(\n",
        "    (date_encoder, date_cols),\n",
        "    (categorical_encoder, categorical_cols),\n",
        "    remainder='passthrough',  # passthrough numerical columns as they are\n",
        ")\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "rfr = RandomForestRegressor()\n",
        "print(rfr.get_params().keys())\n",
        "\n",
        "pipeline = make_pipeline( preprocessor_one_hot, rfr)\n",
        "pipeline_gs = GridSearchCV(pipeline,\n",
        "                          param_grid={'n_estimators': [10, 100]})\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10)\n",
        "pipeline.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'year' is not in list",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c8af9ddcae0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                           param_grid={'n_estimators': [10, 100]})\n\u001b[1;32m     23\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 raise ValueError(\n\u001b[1;32m    467\u001b[0m                     \u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 ) from e\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnlNo_r62t2"
      },
      "source": [
        "rfr.get_params().keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuPx8ad8sWnC"
      },
      "source": [
        "# Merging external data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVDrQkusWnC"
      },
      "source": [
        "The objective in this RAMP data challenge is to find good data that can be correlated to flight traffic. We will use some weather data (saved in `submissions/starting_kit`) to provide an example of how to merge external data in a scikit-learn pipeline.\n",
        "\n",
        "Your external data will need to be included in your submissions folder - see [RAMP submissions](#RAMP-submissions) for more details.\n",
        "\n",
        "First we will define a function that merges the external data to our feature data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eKzdHassWnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b41abce-6339-4853-e233-e66b157f0ebb"
      },
      "source": [
        "# when submitting a kit, the `__file__` variable will corresponds to the\n",
        "# path to `estimator.py`. However, this variable is not defined in the\n",
        "# notebook and thus we must define the `__file__` variable to imitate\n",
        "# how a submission `.py` would work.\n",
        "__file__ = os.path.join('submissions', 'starting_kit', 'estimator.py')\n",
        "filepath = os.path.join(os.path.dirname(__file__), 'external_data.csv')\n",
        "filepath"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'submissions/starting_kit/external_data.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nRafLVAsWnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "f9631d39-514c-4651-9fe1-732587ef1874"
      },
      "source": [
        "pd.read_csv(filepath).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>AirPort</th>\n",
              "      <th>Max TemperatureC</th>\n",
              "      <th>Mean TemperatureC</th>\n",
              "      <th>Min TemperatureC</th>\n",
              "      <th>Dew PointC</th>\n",
              "      <th>MeanDew PointC</th>\n",
              "      <th>Min DewpointC</th>\n",
              "      <th>Max Humidity</th>\n",
              "      <th>Mean Humidity</th>\n",
              "      <th>Min Humidity</th>\n",
              "      <th>Max Sea Level PressurehPa</th>\n",
              "      <th>Mean Sea Level PressurehPa</th>\n",
              "      <th>Min Sea Level PressurehPa</th>\n",
              "      <th>Max VisibilityKm</th>\n",
              "      <th>Mean VisibilityKm</th>\n",
              "      <th>Min VisibilitykM</th>\n",
              "      <th>Max Wind SpeedKm/h</th>\n",
              "      <th>Mean Wind SpeedKm/h</th>\n",
              "      <th>Max Gust SpeedKm/h</th>\n",
              "      <th>Precipitationmm</th>\n",
              "      <th>CloudCover</th>\n",
              "      <th>Events</th>\n",
              "      <th>WindDirDegrees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011-09-01</td>\n",
              "      <td>ATL</td>\n",
              "      <td>35</td>\n",
              "      <td>29</td>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>79</td>\n",
              "      <td>56</td>\n",
              "      <td>32</td>\n",
              "      <td>1022</td>\n",
              "      <td>1019</td>\n",
              "      <td>1017</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011-09-02</td>\n",
              "      <td>ATL</td>\n",
              "      <td>36</td>\n",
              "      <td>29</td>\n",
              "      <td>22</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "      <td>61</td>\n",
              "      <td>46</td>\n",
              "      <td>30</td>\n",
              "      <td>1019</td>\n",
              "      <td>1016</td>\n",
              "      <td>1014</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>24</td>\n",
              "      <td>7</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-09-03</td>\n",
              "      <td>ATL</td>\n",
              "      <td>35</td>\n",
              "      <td>29</td>\n",
              "      <td>23</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>14</td>\n",
              "      <td>64</td>\n",
              "      <td>47</td>\n",
              "      <td>30</td>\n",
              "      <td>1015</td>\n",
              "      <td>1013</td>\n",
              "      <td>1011</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>19</td>\n",
              "      <td>7</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-09-04</td>\n",
              "      <td>ATL</td>\n",
              "      <td>27</td>\n",
              "      <td>24</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>93</td>\n",
              "      <td>72</td>\n",
              "      <td>51</td>\n",
              "      <td>1014</td>\n",
              "      <td>1012</td>\n",
              "      <td>1011</td>\n",
              "      <td>16</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>9</td>\n",
              "      <td>26.0</td>\n",
              "      <td>6.10</td>\n",
              "      <td>6</td>\n",
              "      <td>Rain</td>\n",
              "      <td>139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011-09-05</td>\n",
              "      <td>ATL</td>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>94</td>\n",
              "      <td>91</td>\n",
              "      <td>87</td>\n",
              "      <td>1010</td>\n",
              "      <td>1005</td>\n",
              "      <td>999</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>16</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.00</td>\n",
              "      <td>8</td>\n",
              "      <td>Rain-Thunderstorm</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date AirPort  Max TemperatureC  Mean TemperatureC  Min TemperatureC  \\\n",
              "0  2011-09-01     ATL                35                 29                24   \n",
              "1  2011-09-02     ATL                36                 29                22   \n",
              "2  2011-09-03     ATL                35                 29                23   \n",
              "3  2011-09-04     ATL                27                 24                22   \n",
              "4  2011-09-05     ATL                26                 24                22   \n",
              "\n",
              "   Dew PointC  MeanDew PointC  Min DewpointC  Max Humidity  Mean Humidity  \\\n",
              "0          21              18             14            79             56   \n",
              "1          17              15             14            61             46   \n",
              "2          17              16             14            64             47   \n",
              "3          22              19             16            93             72   \n",
              "4          23              22             20            94             91   \n",
              "\n",
              "   Min Humidity  Max Sea Level PressurehPa  Mean Sea Level PressurehPa  \\\n",
              "0            32                       1022                        1019   \n",
              "1            30                       1019                        1016   \n",
              "2            30                       1015                        1013   \n",
              "3            51                       1014                        1012   \n",
              "4            87                       1010                        1005   \n",
              "\n",
              "   Min Sea Level PressurehPa  Max VisibilityKm  Mean VisibilityKm  \\\n",
              "0                       1017                16                 16   \n",
              "1                       1014                16                 16   \n",
              "2                       1011                16                 16   \n",
              "3                       1011                16                 14   \n",
              "4                        999                16                 13   \n",
              "\n",
              "   Min VisibilitykM  Max Wind SpeedKm/h  Mean Wind SpeedKm/h  \\\n",
              "0                11                  19                    6   \n",
              "1                16                  24                    7   \n",
              "2                16                  19                    7   \n",
              "3                 4                  21                    9   \n",
              "4                 3                  32                   16   \n",
              "\n",
              "   Max Gust SpeedKm/h Precipitationmm  CloudCover             Events  \\\n",
              "0                26.0            0.00           3                NaN   \n",
              "1                34.0            0.00           2                NaN   \n",
              "2                26.0            0.00           4                NaN   \n",
              "3                26.0            6.10           6               Rain   \n",
              "4                45.0           16.00           8  Rain-Thunderstorm   \n",
              "\n",
              "   WindDirDegrees  \n",
              "0             129  \n",
              "1             185  \n",
              "2             147  \n",
              "3             139  \n",
              "4             149  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZpdp_bisWnD"
      },
      "source": [
        "def _merge_external_data(X):\n",
        "    filepath = os.path.join(\n",
        "        os.path.dirname(__file__), 'external_data.csv'\n",
        "    )\n",
        "    \n",
        "    X = X.copy()  # to avoid raising SettingOnCopyWarning\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Parse date to also be of dtype datetime\n",
        "    data_weather = pd.read_csv(filepath, parse_dates=[\"Date\"])\n",
        "\n",
        "    X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC']]\n",
        "    X_weather = X_weather.rename(\n",
        "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'})\n",
        "    X_merged = pd.merge(\n",
        "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
        "    )\n",
        "    return X_merged\n",
        "\n",
        "data_merger = FunctionTransformer(_merge_external_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZNIYmDOsWnD"
      },
      "source": [
        "Double check that our function works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFaRbmgYsWnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1852a5ea-e5c7-4aa5-a890-b728c8ad2226"
      },
      "source": [
        "data_merger.fit_transform(X).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DateOfDeparture</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>WeeksToDeparture</th>\n",
              "      <th>std_wtd</th>\n",
              "      <th>Max TemperatureC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-06-19</td>\n",
              "      <td>ORD</td>\n",
              "      <td>DFW</td>\n",
              "      <td>12.875000</td>\n",
              "      <td>9.812647</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-09-10</td>\n",
              "      <td>LAS</td>\n",
              "      <td>DEN</td>\n",
              "      <td>14.285714</td>\n",
              "      <td>9.466734</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-10-05</td>\n",
              "      <td>DEN</td>\n",
              "      <td>LAX</td>\n",
              "      <td>10.863636</td>\n",
              "      <td>9.035883</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-10-09</td>\n",
              "      <td>ATL</td>\n",
              "      <td>ORD</td>\n",
              "      <td>11.480000</td>\n",
              "      <td>7.990202</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-02-21</td>\n",
              "      <td>DEN</td>\n",
              "      <td>SFO</td>\n",
              "      <td>11.450000</td>\n",
              "      <td>9.517159</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  DateOfDeparture Departure Arrival  WeeksToDeparture   std_wtd  \\\n",
              "0      2012-06-19       ORD     DFW         12.875000  9.812647   \n",
              "1      2012-09-10       LAS     DEN         14.285714  9.466734   \n",
              "2      2012-10-05       DEN     LAX         10.863636  9.035883   \n",
              "3      2011-10-09       ATL     ORD         11.480000  7.990202   \n",
              "4      2012-02-21       DEN     SFO         11.450000  9.517159   \n",
              "\n",
              "   Max TemperatureC  \n",
              "0                34  \n",
              "1                33  \n",
              "2                22  \n",
              "3                27  \n",
              "4                16  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp50YKxEsWnD"
      },
      "source": [
        "Use `FunctionTransformer` to make our function compatible with scikit-learn API:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRBgQCssWnD"
      },
      "source": [
        "We can now assemble our pipeline using the same `data_merger` and `preprocessor` as above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvqjC8fsWnD"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "date_encoder = FunctionTransformer(_encode_dates)\n",
        "date_cols = [\"DateOfDeparture\"]\n",
        "\n",
        "categorical_encoder = OrdinalEncoder()\n",
        "categorical_cols = [\"Arrival\", \"Departure\"]\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (date_encoder, date_cols),\n",
        "    (categorical_encoder, categorical_cols),\n",
        "    remainder='passthrough',  # passthrough numerical columns as they are\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X6yKUlGsWnD"
      },
      "source": [
        "n_estimators = 10\n",
        "max_depth = 10\n",
        "max_features = 10\n",
        "\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=n_estimators, max_depth=max_depth, max_features=max_features\n",
        ")\n",
        "\n",
        "pipeline = make_pipeline(data_merger, preprocessor, regressor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Twag2u5sWnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e447555-fd19-4f54-aa65-9b567a2fc954"
      },
      "source": [
        "scores = cross_val_score(\n",
        "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
        ")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\n",
        "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n",
            "RMSE: 0.6322 +/- 0.0153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciQIamMCK7zk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjrPTHMzK8XC"
      },
      "source": [
        "# le cas general est la"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDumXxLk9WAC"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "def _merge_external_data(X):\n",
        "    filepath = os.path.join(\n",
        "        os.path.dirname(__file__), 'external_data.csv'\n",
        "    )\n",
        "  \n",
        "    X = X.copy()  # to avoid raising SettingOnCopyWarning\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Parse date to also be of dtype datetime\n",
        "    data_weather = pd.read_csv(filepath, parse_dates=[\"Date\"])\n",
        "\n",
        "    X_weather = data_weather[['Date', 'AirPort', 'Mean TemperatureC']]\n",
        "    X_weather = X_weather.rename(\n",
        "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'})\n",
        "    X_merged = pd.merge(\n",
        "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
        "    )\n",
        "    return X_merged\n",
        "\n",
        "\n",
        "\n",
        "def _encode_dates(X):\n",
        "    # Make sure that DateOfDeparture is of dtype datetime\n",
        "    X = X.copy()  # modify a copy of X\n",
        "    X.loc[:, \"DateOfDeparture\"] = pd.to_datetime(X['DateOfDeparture'])\n",
        "    # Encode the date information from the DateOfDeparture columns\n",
        "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
        "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
        "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
        "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
        "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.isocalendar().week\n",
        "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
        "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
        "    )\n",
        "    # Finally we can drop the original columns from the dataframe\n",
        "    return X.drop(columns=[\"DateOfDeparture\"])\n",
        " \n",
        "def get_estimator():\n",
        "\n",
        "    data_merger = FunctionTransformer(_merge_external_data)\n",
        "    date_encoder = FunctionTransformer(_encode_dates)    \n",
        "    date_cols = [\"DateOfDeparture\"]\n",
        " \n",
        "    categorical_cols = [\"Arrival\", \"Departure\"]\n",
        "    categorical_encoder = OrdinalEncoder(categorical_cols)    \n",
        " \n",
        "    preprocessor = make_column_transformer(\n",
        "        (date_encoder, date_cols),\n",
        "        (categorical_encoder, categorical_cols),\n",
        "        remainder='passthrough',  # passthrough numerical columns as they are\n",
        "      )\n",
        " \n",
        "    n_estimators = 200\n",
        "    max_depth = 15\n",
        "    max_features = 10\n",
        "\n",
        "    # regressor = RandomForestRegressor(\n",
        "    #   \tn_estimators=n_estimators, max_depth=max_depth, max_features=max_features\n",
        "\t  # )\n",
        "    regressor = GradientBoostingRegressor(\n",
        "      max_depth=8,\n",
        "      n_estimators=3000,\n",
        "      learning_rate=0.05\n",
        "        )\n",
        " \n",
        "    return make_pipeline(preprocessor, regressor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A1rB1TI9sqG"
      },
      "source": [
        "pipeline = make_pipeline(get_estimator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb044JRA9mzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0349053c-7b4d-4565-a4c1-524d8c6f0569"
      },
      "source": [
        "scores = cross_val_score(\n",
        "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
        ")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\n",
        "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: iteration over a 0-d array\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: iteration over a 0-d array\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: iteration over a 0-d array\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: iteration over a 0-d array\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE: nan +/- nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "TypeError: iteration over a 0-d array\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO7vnQT-sWnD"
      },
      "source": [
        "## Feature importances\n",
        "\n",
        "We can check the feature importances using the function [`sklearn.inspection.permutation_importances`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html). Since the first step of our pipeline adds the new external feature `Max TemperatureC`, we want to apply this transformation after adding `Max TemperatureC`, to check the importances of all features. Indeed, we can perform `sklearn.inspection.permutation_importances` at any stage of the pipeline, as we will see later on.\n",
        "\n",
        "\n",
        "The code below:\n",
        "\n",
        "* performs `transform` on the first step of the pipeline (`pipeline[0]`) producing the transformed train (`X_train_augmented`) and test (`X_test_augmented`) data\n",
        "* the transformed data is used to fit the pipeline from the second step onwards\n",
        "\n",
        "Note that pipelines can be sliced. `pipeline[0]` obtains the first step (tuple) of the pipeline. You can further slice to obtain either the transformer/estimator (first item in each tuple) or column list (second item within each tuple) inside each tuple. For example `pipeline[0][0]` obtains the transformer of the first step of the pipeline (first item of the first tuple)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7yyz5rvngOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d305bd67-54e6-4ddd-e3e6-088a5c243520"
      },
      "source": [
        "pipeline[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
              "                    func=<function _merge_external_data at 0x7f25f7889f28>,\n",
              "                    inv_kw_args=None, inverse_func=None, kw_args=None,\n",
              "                    validate=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnDUiUgTsWnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "12864d42-2043-40a3-ea32-e15952d55472"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42\n",
        ")\n",
        "\n",
        "merger = pipeline[0][0]\n",
        "X_train_augmented = merger.transform(X_train)\n",
        "X_test_augmented = merger.transform(X_test)\n",
        "\n",
        "predictor = pipeline[1:]\n",
        "predictor.fit(X_train_augmented, y_train).score(X_test_augmented, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_test_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pipeline slicing only supports a step of 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOH5AeJ8sWnE"
      },
      "source": [
        "With the fitted pipeline, we can now use `permutation_importance` to calculate feature importances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SxAo9z5sWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5a597f0d-833a-4116-8add-116b6f830fc8"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "feature_importances = permutation_importance(\n",
        "    predictor, X_train_augmented, y_train, n_repeats=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m feature_importances = permutation_importance(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfo8N6O_sWnE"
      },
      "source": [
        "Here, we plot the permutation importance using the training set. The higher the value, more important the feature is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbmG3WTAsWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e2317dc3-2a15-4c57-ca84-f965e227dfb8"
      },
      "source": [
        "sorted_idx = feature_importances.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
        "           vert=False, labels=X_train_augmented.columns[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (train set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ax.boxplot(feature_importances.importances[sorted_idx].T,\n\u001b[1;32m      5\u001b[0m            vert=False, labels=X_train_augmented.columns[sorted_idx])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_importances' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXP8-ioysWnE"
      },
      "source": [
        "We can replicate the same processing on the test set and see if we can observe the same trend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CErvkdZsWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "05d6e24b-4474-4df9-e06f-1481217d0aff"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "feature_importances = permutation_importance(\n",
        "    predictor, X_test_augmented, y_test, n_repeats=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m feature_importances = permutation_importance(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0xEpH_jsWnE"
      },
      "source": [
        "sorted_idx = feature_importances.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
        "           vert=False, labels=X_test_augmented.columns[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK8GrL67sWnE"
      },
      "source": [
        "With the current version of scikit-learn, it is not handy but still possible to check the feature importances at the latest stage of the pipeline (once all features have been preprocessed).\n",
        "\n",
        "The difficult part is to get the name of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVUudTNHsWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "9bc117c2-f69e-4486-96d0-8f054970e1c3"
      },
      "source": [
        "preprocessor = pipeline[:-1]\n",
        "predictor = pipeline[-1]\n",
        "\n",
        "X_train_augmented = preprocessor.transform(X_train)\n",
        "X_test_augmented = preprocessor.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pipeline slicing only supports a step of 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSx9E4dFps44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "35a7661a-fe02-47ea-eb35-1b4803c50bc9"
      },
      "source": [
        "pipeline[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;31m# Not an int, try get step by name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wP_vYIxsWnE"
      },
      "source": [
        "Let's find out the feature names (in the future, scikit-learn will provide a `get_feature_names` function to handle this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HonDQvHsWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2e1e9ae9-cc26-4e16-c5cf-1fd52de95196"
      },
      "source": [
        "date_cols_name = (date_encoder.transform(X_train[date_cols])\n",
        "                              .columns.tolist())\n",
        "categorical_cols_name = categorical_cols\n",
        "numerical_cols_name = (pipeline[0].transform(X_train)\n",
        "                                  .columns[pipeline[1].transformers_[-1][-1]]\n",
        "                                  .tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['DateOfDeparture', 'year', 'month', 'day', 'weekday', 'week', 'n_days'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                               .columns.tolist())\n\u001b[1;32m      3\u001b[0m \u001b[0mcategorical_cols_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m numerical_cols_name = (pipeline[0].transform(X_train)\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   .tolist())\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# XXX: Handling the None case means we can't use if_delegate_has_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GradientBoostingRegressor' object has no attribute 'transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w5kvjxysWnE"
      },
      "source": [
        "feature_names = np.array(\n",
        "    date_cols_name + categorical_cols_name + numerical_cols_name\n",
        ")\n",
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8YrXDT2sWnE"
      },
      "source": [
        "We can repeat the previous processing at this finer grain, where the transformed date columns are included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xELCHnCosWnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "26bdeaf5-64af-4175-a287-b3a1a673bf58"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "feature_importances = permutation_importance(\n",
        "    predictor, X_train_augmented, y_train, n_repeats=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/air_passengers/submissions/starting_kit/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m feature_importances = permutation_importance(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l10TTFFrsWnE"
      },
      "source": [
        "Here, we plot the permutation importance using the training set. Basically, higher the value, more important is the feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr0bCCBzsWnE"
      },
      "source": [
        "sorted_idx = feature_importances.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
        "           vert=False, labels=feature_names[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (train set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr7rq9qesWnE"
      },
      "source": [
        "We can replicate the same processing on the test set and see if we can observe the same trend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBkaXy0VsWnE"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "feature_importances = permutation_importance(\n",
        "    predictor, X_test_augmented, y_test, n_repeats=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t2k9x7PsWnE"
      },
      "source": [
        "sorted_idx = feature_importances.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
        "           vert=False, labels=feature_names[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLNgFw2dsWnF"
      },
      "source": [
        "## Submission\n",
        "\n",
        "To submit your code, you can refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ePmY2RvLUaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}